{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mobility_flow_prediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMlsoqj9It69n1gFGrYZGgo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","In this notebook, I present two models of mobility flow forecasting alghorithms.\n","\n","The dataset that has been used is taken from a mobility flow dataset provided by Regione Emilia-Romagna for research purposes to the Complex System Group - City Science Lab of the University of Bologna structured as follows:\n","\n","| DateTime  | Road 1 | Road 2 | ... | Road N |\n","| ----------- | ----------- | | ----------- | ----------- |   \n","| 2020-02-01 00:00:00+00:00      | # vehicles       | | ... # vehicles ...      | | ...|  \n","| 2020-02-01 00:15:00+00:00      | # vehicles       | | ... # vehicles ...      | | ...| \n","|...| | ... | ... | |\n","\n","Hence, in this fifteen-minutes-sampled-timeseries database, it is shown the amount of vehicles that drives on a specific road. The goal of the present notebook is to predict these numbers at best for each road.\n","\n","The first implementation is a Multilayer Perceptron Model (ANN) composed by 3 + 1 levels, while the second alghorithm that has been applied to get predictions is Prophet, an open source software released by Facebook's Code Data Science Team. Both results have been compared to the observed data distribution and I have find out that the first implementation gets better results probably due to the fact that Prophet works best with time series that have strong seasonal effects and several seasons of historical data, while for this notebook a month of data has been used. "],"metadata":{"id":"LnAOO-z3_tIV"}},{"cell_type":"markdown","source":["# Initializations"],"metadata":{"id":"o2mez9fIEE4A"}},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"5wXoj_j_fdGI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKxWR1oCZ-WB"},"outputs":[],"source":["import os\n","import glob\n","import keras\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","%matplotlib inline"]},{"cell_type":"markdown","source":["## Install useful packages"],"metadata":{"id":"Y4K5qcIixDQa"}},{"cell_type":"code","source":["!pip install pystan~=2.14\n","!pip install fbprophet"],"metadata":{"id":"3RA3PAuRwhzP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Libraries for forcasting"],"metadata":{"id":"ZeBBqpCsxJLi"}},{"cell_type":"code","source":["import fbprophet as Prophet\n","import time"],"metadata":{"id":"jkZy9FjSwEO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot Tools"],"metadata":{"id":"zq_s7Kz76sz5"}},{"cell_type":"markdown","source":["Here I define a python class that simplifies the visualization of results by setting a statistical threshold."],"metadata":{"id":"ofEVzKWCXoDU"}},{"cell_type":"code","source":["class cg():\n","  LOW  = 1\n","  AVE  = 2\n","  HIGH = 3\n","\n","kpi_thresh = {\n","  cg.LOW  : 0.20,\n","  cg.AVE  : 0,\n","  cg.HIGH : 0.80\n","}\n","\n","kpi_colors = {\n","  cg.LOW  : 'blue',\n","  cg.AVE  : 'white',\n","  cg.HIGH : 'red'\n","}"],"metadata":{"id":"MKsO-M3g6w04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare DataSet"],"metadata":{"id":"CTfGgumyfYyg"}},{"cell_type":"markdown","source":["## Read Dataset "],"metadata":{"id":"WGfbr0KTfhVB"}},{"cell_type":"code","source":["# Set True if importing file from Google Drive (Firefox Browser)\n","GoogleDrive = True"],"metadata":{"id":"C188Y016DtXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importing the whole dataset from Google Drive or from local folder and unzipping it into a data folder named \"dataset\"."],"metadata":{"id":"2pSeb3pjaoWD"}},{"cell_type":"code","source":["if GoogleDrive:\n","  path_to_file = f'/gdrive/MyDrive/Colab_Notebooks/dataset.zip'\n","  from google.colab import drive\n","  drive.mount('/gdrive')\n","  zip_file = zipfile.ZipFile(path_to_file, 'r')\n","  zip_file.extractall()\n","else:\n","  from google.colab import files\n","  upload = files.upload()\n","  with zipfile.ZipFile(f'/content/dataset.zip', 'r') as zip_file:\n","    zip_file.extractall()\n","\n","zip_file.close()"],"metadata":{"id":"Zn3tpDJoctdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reading all the .csv files in the unzipped folder and creating a unique dataframe indexed by times.\n","\n","Here, in the columns there are the roads that label the street network and the column values are the quarter hour's counter.  "],"metadata":{"id":"vL8V5pNKa3QU"}},{"cell_type":"code","source":["path = f'dataset/'\n","all_files = glob.glob(os.path.join(path , \"*.csv\"))\n","li = []\n","\n","for filename in all_files:\n","  df = pd.read_csv(filename, sep=\";\", index_col=0, header=0, parse_dates=True).tz_convert(\"Europe/Rome\").tz_localize(None)\n","  li.append(df)\n","\n","frame = pd.concat(li, axis=0, ignore_index=False)\n","frame = frame.T.groupby([s.split('_')[0] for s in frame.T.index.values]).sum().T.sort_index()\n","frame = frame[['174']] # I pick up one road on which focusing the analysis.\n","frame = frame.rename(columns={x:f'road_{y}' for x,y in zip(frame.columns,range(0,len(frame.columns)))})"],"metadata":{"id":"SnbmSwOJdBUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset for Multilayer Perceptron"],"metadata":{"id":"xdRwY-quanvi"}},{"cell_type":"markdown","source":["Here I select a subset of the data in order to avoid significant discontinuities in the timeseries. Moreover, I picked up one road on which focusing the analysis."],"metadata":{"id":"Q-ZKW9xTdL_I"}},{"cell_type":"code","source":["start_date = pd.to_datetime('2021-09-30 00:00:00')\n","stop_date = pd.to_datetime('2021-10-31 23:59:59')\n","mask_multiperceptron = (frame.index >= start_date) & (frame.index <= stop_date)\n","df_mp = frame.loc[mask_multiperceptron]\n","df_mp = df_mp.iloc[:, 0:1]"],"metadata":{"id":"1PfWRcOAavIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset visualization\n","- On the x-axis the datetime is shown;\n","- On the y-axis the number of vehicles that crossed that road in 15 minutes. "],"metadata":{"id":"A-Sw_S3MdzVc"}},{"cell_type":"code","source":["ts = [ t.timestamp() for t in df_mp.index ]\n","ts_lbl = [ t.strftime('%a %b %d %Y') for t in df_mp.index ]\n","tick_us = 96\n","\n","fig, ax = plt.subplots(1,1, figsize=(20,8), sharex=True)\n","ax.set_xlabel('Daytime [Wday Mon DD YYYY]')\n","ax.set_ylabel('Counter [# of vehicles]')\n","ax.set_title(f'Original Dataset', size=18)\n","ax.plot(ts, df_mp, c='blue', label='Observed data')\n","ax.set_xticks(ts[::tick_us])\n","ax.set_xticklabels(ts_lbl[::tick_us], rotation=45, ha='right')\n","ax.grid(which='major', linestyle='-')\n","ax.grid(which='minor', linestyle='--')\n","ax.legend()\n","\n","plt.show()"],"metadata":{"id":"_4jh6-pDgkDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset for Prophet"],"metadata":{"id":"HDOdMhP-a6RB"}},{"cell_type":"markdown","source":["Using Prophet, I split the whole dataset into two classes: the first one is the training-dataset and the second one is the test-dataset. Moreover, Prophet requires specific names for the dataframe columns: the datatime column must be labelled as \"*ds*\" while the counter column as \"*y*\"."],"metadata":{"id":"24B8fqo-eZ5U"}},{"cell_type":"code","source":["df_tf = frame.iloc[:, 0:1]\n","start_date_train = pd.to_datetime('2021-09-30 00:00:00')\n","stop_date_train = pd.to_datetime('2021-10-27 23:59:59')\n","stop_date_test = pd.to_datetime('2021-10-31 23:59:59')\n","mask_period = (df_tf.index >= start_date_train) & (df_tf.index <= stop_date_test)\n","mask_train = (df_tf.index >= start_date_train) & (df_tf.index <= stop_date_train)\n","mask_test = (df_tf.index > stop_date_train) & (df_tf.index <= stop_date_test)\n","\n","df_train = df_tf.loc[mask_train].reset_index(level=0)\n","df_test = df_tf.loc[mask_test].reset_index(level=0)\n","\n","df_train = df_train.rename(columns = {df_train.columns[0]:'ds', df_train.columns[1]:'y'})\n","df_test = df_test.rename(columns = {df_test.columns[0]:'ds', df_test.columns[1]:'y'})"],"metadata":{"id":"kZ3WGWnPa52e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this block and in the following I show the data population used for the train and for the test of the alghoritm."],"metadata":{"id":"DsqukrW9foPR"}},{"cell_type":"code","source":["# Data Train Visualization\n","ts3 = [ t.timestamp() for t in pd.to_datetime(df_train.ds) ]\n","ts_lbl3 = [ t.strftime('%a %b %d %Y') for t in pd.to_datetime(df_train.ds) ]\n","tick_us3 = 96\n","\n","fig, ax3 = plt.subplots(1,1, figsize=(20,8))\n","ax3.set_xlabel('Daytime [Wday Mon DD YYYY]')\n","ax3.set_ylabel('Counter [# of vehicles]')\n","ax3.set_title(f'Training Dataset', size=18)\n","ax3.plot(ts3, df_train['y'], c='blue', label='Training Dataset')\n","ax3.set_xticks(ts3[::tick_us3])\n","ax3.set_xticklabels(ts_lbl3[::tick_us3], rotation=45, ha='right')\n","ax3.grid(which='major', linestyle='-')\n","ax3.grid(which='minor', linestyle='--')\n","ax3.legend()\n","\n","plt.show()"],"metadata":{"id":"AV_N8duTVtz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Test Visualization\n","ts2 = [ t.timestamp() for t in pd.to_datetime(df_test.ds) ]\n","ts_lbl2 = [ t.strftime('%a %b %d %Y %H:%M') for t in pd.to_datetime(df_test.ds) ]\n","tick_us2 = 12\n","\n","fig, ax2 = plt.subplots(1,1, figsize=(20,8), sharey=True)\n","ax2.set_xlabel('Daytime [Wday Mon DD YYYY HH:MM]')\n","ax2.set_ylabel('Counter [# of vehicles]')\n","ax2.set_title(f'Test Dataset', size=18)\n","ax2.plot(ts2, df_test['y'], c='blue', label='Test Dataset')\n","ax2.set_xticks(ts2[::tick_us2])\n","ax2.set_xticklabels(ts_lbl2[::tick_us2], rotation=45, ha='right')\n","ax2.grid(which='major', linestyle='-')\n","ax2.grid(which='minor', linestyle='--')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"1kTa_e8WR1kz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multilayer Perceptron"],"metadata":{"id":"WDIGPmDYjwqC"}},{"cell_type":"markdown","source":["## Reshape Dataset and Features Scaling"],"metadata":{"id":"qmK3cdMNilj2"}},{"cell_type":"markdown","source":["Here I reshape the dataset by adding a windows size of 96 (number of quarters of an hour in a day) by rescaling it with scikit-learn function MinMaxScaler to simplify the computation step."],"metadata":{"id":"0LnXOBDWgC3G"}},{"cell_type":"code","source":["def create_dataset(dataset, window=1):\n","    dataX, dataY= [], []\n","    for i in range(len(dataset)-window-1):\n","        a = dataset[i:(i+window),0]\n","        dataX.append(a)\n","        dataY.append(dataset[i+window,0])\n","    return np.array(dataX), np.array(dataY)\n","\n","scaler = MinMaxScaler(feature_range=(0,1))\n","df_mp = scaler.fit_transform(np.array(df_mp).reshape(-1,1))"],"metadata":{"id":"_WDcczHDiqXd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Split Train and Test"],"metadata":{"id":"XByH-XAzjQ59"}},{"cell_type":"markdown","source":["I will use the 80% of data to train the model and the remaining 20% to test it."],"metadata":{"id":"708QE6eNYeq-"}},{"cell_type":"code","source":["training_size = int(len(df_mp)*0.80)\n","test_size = len(df_mp)-training_size\n","train_data, test_data = df_mp[0:training_size,:], df_mp[training_size:len(df_mp),:1]\n","\n","window = 96\n","X_train,y_train = create_dataset(train_data,window)\n","X_test, y_test = create_dataset(test_data,window)"],"metadata":{"id":"ENlq-Zw_jVqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimization Method and Model Training"],"metadata":{"id":"Q7nylVuLMeE8"}},{"cell_type":"markdown","source":["Here I define the ANN's architecture: in order to train the model I implement a 3 + 1 layers each with a rectified linear unit as activation function."],"metadata":{"id":"LUoDOZu7XzGp"}},{"cell_type":"code","source":["model = Sequential()\n","model.add(Dense(40, input_dim=window, activation='relu'))\n","model.add(Dense(50, activation='relu'))\n","model.add(Dense(40, activation='relu'))\n","model.add(Dense(1))"],"metadata":{"id":"XnccPV5sj0sp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Choosing the optimizer function from Keras' package of Tensorflow. Here I decided to use the adaptive gradient algorithm which is adapted relative to how frequently a parameter gets updated during training."],"metadata":{"id":"xaPONY2Gl1eR"}},{"cell_type":"code","source":["opt  = tf.keras.optimizers.Adagrad(learning_rate = 0.05)\n","model.compile(optimizer=opt ,loss='mean_squared_error')\n","model.summary()"],"metadata":{"id":"wKkJeC8Cj2jF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training of the model to minimize the loss function. "],"metadata":{"id":"hF0vEVUuprXm"}},{"cell_type":"code","source":["model.fit(X_train, y_train, epochs=300, batch_size=10, verbose=1)"],"metadata":{"id":"oIij7MPqj86D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction"],"metadata":{"id":"wMzH-_JokDaL"}},{"cell_type":"markdown","source":["Getting the prediction."],"metadata":{"id":"Xq_2GVRdqV4D"}},{"cell_type":"code","source":["train_predict = model.predict(X_train)\n","test_predict = model.predict(X_test)"],"metadata":{"id":"PDeduIOMkB5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_predict = scaler.inverse_transform(train_predict)\n","test_predict = scaler.inverse_transform(test_predict)\n","y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n","y_test = scaler.inverse_transform(y_test.reshape(-1, 1))"],"metadata":{"id":"HR51gyFukGv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_predict = train_predict.astype(int)\n","test_predict = test_predict.astype(int)\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)"],"metadata":{"id":"pR-3XYPkkLeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accuracy Measures"],"metadata":{"id":"vY918MSOmn7v"}},{"cell_type":"markdown","source":["Computing the accuracy measures of both training and test data.\n","Here we can see that the R-squared scores of 98% test of accuracy"],"metadata":{"id":"5vizbtMSqhOX"}},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n","print('** Train Accuracy **')\n","print('RMSE-train:',np.sqrt(mean_squared_error(y_train,train_predict)))\n","print('MAE-train:',mean_absolute_error(y_train,train_predict))\n","print('R_2-train:',r2_score(y_train,train_predict))\n","print('\\n** Test Accuracy **')\n","print('RMSE-test:',np.sqrt(mean_squared_error(y_test,test_predict)))\n","print('MAE-test:',mean_absolute_error(y_test,test_predict))\n","print('R_2-test:',r2_score(y_test,test_predict))"],"metadata":{"id":"hFbTm80smqUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot\n","###Observed Dataset - Training Set - Test Set"],"metadata":{"id":"XSSUARhfm1lR"}},{"cell_type":"markdown","source":["Here I present a visualization of the three sets of data. \n","- The original dataset, the one that has been observed, is marked in blue; \n","- The dataset that has been used for the training part of the alghoritm is marked in red; \n","- Finally, the test dataset is labelled in black and it can be said that the agreement with the observed data is remarkable."],"metadata":{"id":"tdyvdta6rBLG"}},{"cell_type":"code","source":["trainPredictPlot = np.empty_like(df_mp)\n","trainPredictPlot[:, :] = np.nan\n","trainPredictPlot[window:len(train_predict)+window, :] = train_predict\n","\n","testPredictPlot = np.empty_like(df_mp)\n","testPredictPlot[:, :] = np.nan\n","testPredictPlot[len(train_predict)+(window*2)+1:len(df_mp)-1, :] = test_predict\n","\n","fig, ax = plt.subplots(1,1, figsize=(20,8))\n","ax.set_xlabel('Daytime [Wday Mon DD YYYY]')\n","ax.set_ylabel('Counter [# of vehicles]')\n","ax.set_title(f'Results Plot\\nObserved Dataset - Training Set - Test Set', size=18)\n","ax.plot(ts, scaler.inverse_transform(df_mp),'-', c='blue', label='Observed Traffic Data')\n","ax.plot(ts, trainPredictPlot,'-.', c='red', label='Training dataset')\n","ax.plot(ts, testPredictPlot,'--', c= 'black', label='Test dataset')\n","ax.set_xticks(ts[::tick_us])\n","ax.set_xticklabels(ts_lbl[::tick_us], rotation=45, ha='right')\n","ax.grid(which='major', linestyle='-')\n","ax.grid(which='minor', linestyle='--')\n","ax.legend()\n","plt.show()"],"metadata":{"id":"LE_GCd8ym39o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare Data Comparison"],"metadata":{"id":"hziwL11SKh2v"}},{"cell_type":"markdown","source":["In this section I focus on the period used for the test analysis and I show a comparison between the data observed and the predicted data by the multilayer perceptron. \n","\n","Firstly, I compute the difference between the original data and the forecasted data, then the quantile has been computed assuming two levels of thresholds as defined in the class cg()."],"metadata":{"id":"HqAB24fA7vZk"}},{"cell_type":"code","source":["datetime = pd.to_datetime([int(t) for t in ts], unit='s')\n","\n","df2 = pd.DataFrame(index=datetime, columns=['test_predict_plot', 'original_data'])\n","df2['test_predict_plot'] = testPredictPlot\n","df2['original_data'] = scaler.inverse_transform(df_mp)\n","mask_compare = (df2.index > stop_date_train) & (df2.index <= stop_date_test)\n","df2 = df2.loc[mask_compare]\n","dif = df2.test_predict_plot - df2.original_data\n","\n","ave = dif.mean()\n","std = dif.std()\n","thresh_up = dif.quantile(kpi_thresh[cg.HIGH])\n","thresh_down = dif.quantile(kpi_thresh[cg.LOW])\n","df2['loc_diff'] = dif\n","df2['loc_kpi'] = cg.LOW\n","df2.loc[ df2.loc_diff > thresh_down, 'l_kpi'] = cg.AVE\n","df2.loc[ df2.loc_diff > thresh_up, 'l_kpi'] = cg.HIGH"],"metadata":{"id":"crbpUjToCE7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Plot - Difference Results"],"metadata":{"id":"1KMKtgAvKpPi"}},{"cell_type":"markdown","source":["Here I graphically present the results of the comparison.\n","\n","In the first plot, the observed traffic data is shown in blue colour while the forcasted data using the perceptron alghoritm are shown in red; in this panel one can observe that the two series follow a similar trend, hence the prediction is very accurate. \n","\n","The differences between these two values are shown in the second panel: the majority of points can be found in the area between the 20th and the 80th percentile bands."],"metadata":{"id":"1j_Q9Ki9_cp-"}},{"cell_type":"code","source":["ts3 = [ t.timestamp() for t in df2.index ]\n","ts_lbl3 = [ t.strftime('%a %b %d %Y %H:%M') for t in df2.index ]\n","tick_us3 = 12\n","fig, axs = plt.subplots(2,1, figsize=(20,8), sharex=True)\n","ax3 = axs[0]\n","ax3.set_title(f'Comparison between Observed Traffic and Predicted Traffic Data')\n","ax3.plot(ts3, df2.original_data,'-', c='blue', label='Observed Traffic Data')\n","ax3.plot(ts3, df2.test_predict_plot,'-.', c='red', label='Predicted Traffic Data [Multilayer P]')\n","ax3.set_ylabel('Counter [# of vehicles]')\n","ax3.grid(which='major', linestyle='-')\n","ax3.grid(which='minor', linestyle='--')\n","ax3.legend()\n","\n","ax3 = axs[1]\n","ax3.set_title(f'Statistical Analysis')\n","ax3.set_xticks(ts3[::tick_us3])\n","ax3.set_xticklabels(ts_lbl3[::tick_us3], rotation=45, ha='right')\n","ax3.plot(ts3, df2['loc_diff'],'-o', c='blue', label='Differences')\n","ax3.axhspan(ax3.get_ylim()[0], thresh_down, facecolor=kpi_colors[cg.LOW] , alpha=0.3, label=f'LOW < {kpi_thresh[cg.LOW]} centile')\n","ax3.axhspan(thresh_down, thresh_up, facecolor=kpi_colors[cg.AVE] , alpha=0.3)\n","ax3.axhspan(thresh_up, ax3.get_ylim()[1], facecolor=kpi_colors[cg.HIGH] , alpha=0.3, label=f'HIGH > {kpi_thresh[cg.HIGH]} centile')\n","\n","ax3.grid()\n","ax3.legend()\n","ax3.set_xlabel(f'Daytime [Wday Mon DD YYYY HH:MM]')\n","ax3.set_ylabel('Local Differences')\n","plt.suptitle(f'Multilayer Perceptron Results', size=18 )\n","plt.show()"],"metadata":{"id":"Qu8-ZabzK4E1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prophet"],"metadata":{"id":"I5Tf1khGiX_q"}},{"cell_type":"markdown","source":["In this section, a different forecast alghoritm is applied to the time-series data. \n","\n","I am using a procedure implemented by Facebook's team known as Prophet (https://facebook.github.io/prophet/). Facebook Prophet is an open-source algorithm for generating time-series models that is essentially based over the sum of three functions of time plus an error term: growth $g(t)$, seasonality $s(t)$, holidays $h(t)$ , and error $e_t$:\n","> $y(t) = g(t) + s(t) + h(t) + e_t$\n","\n"],"metadata":{"id":"lcVKgH9qDBfx"}},{"cell_type":"markdown","source":["## Prediction"],"metadata":{"id":"nrFvx7DB_VMF"}},{"cell_type":"markdown","source":["Usually, time series have abrupt changes in their trajectories and Prophet, in the growth function. Prophet will automatically detect these so called *changepoints* and will allow the trend to adapt appropriately."],"metadata":{"id":"UY5AdM5hii-1"}},{"cell_type":"code","source":["t_2 = time.time()\n","m = Prophet.Prophet(changepoint_prior_scale=0.01, yearly_seasonality=False)\n","\n","test_time = df_test.iloc[:,0:1]\n","forecast = m.fit(df_train).predict(test_time)\n","t_3 = time.time()\n","\n","delta_t = t_3-t_2"],"metadata":{"id":"i8EbMT1vxZtL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot"],"metadata":{"id":"VShH_i4Y-e0Q"}},{"cell_type":"markdown","source":["From the following plots we can state that the model has a good agreement with the trend and the seasonal componets of the observed data."],"metadata":{"id":"Oi-HjjzUp4ek"}},{"cell_type":"code","source":["m.plot(forecast)\n","m.plot_components(forecast)"],"metadata":{"id":"9rutIk0U-h25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accuracy Measures"],"metadata":{"id":"cnhTdcRb-nEw"}},{"cell_type":"markdown","source":["Even if the root mean squared error between the original population of data and the forcasted one is very high, the R squared scored indicates a good correlation between the two sets."],"metadata":{"id":"5Trgv4GoqjZs"}},{"cell_type":"code","source":["rmse = np.sqrt(mean_squared_error(df_test['y'], forecast['yhat'].astype(int)))\n","print(f'Root Mean Squared Error :{rmse}')\n","r_sq = r2_score(df_test['y'],forecast['yhat'].astype(int))\n","print(f'R Squared Score : {r_sq}')\n","print(f'Time Required: {delta_t} seconds')"],"metadata":{"id":"hfVIhgda2g52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare Data Comparison\n"],"metadata":{"id":"vhejQ97v4lTV"}},{"cell_type":"code","source":["df_compared = df_test.copy()\n","df_compared['pred'] = forecast['yhat'].values.astype(int)\n","df_compared.set_index('ds', inplace=True)\n","dif2 = df_compared.pred - df_compared.y\n","\n","ave2 = dif2.mean()\n","std2 = dif2.std()\n","thresh_up2 = dif2.quantile(kpi_thresh[cg.HIGH])\n","thresh_down2 = dif2.quantile(kpi_thresh[cg.LOW])\n","df_compared['loc_diff'] = dif2\n","df_compared['loc_kpi'] = cg.LOW\n","df_compared.loc[ df_compared.loc_diff > thresh_down2, 'l_kpi'] = cg.AVE\n","df_compared.loc[ df_compared.loc_diff > thresh_up2, 'l_kpi'] = cg.HIGH"],"metadata":{"id":"oQG-fo8U4psA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Plot - Difference Results"],"metadata":{"id":"l_5HvcGaC49W"}},{"cell_type":"markdown","source":["Here I show the graphical differences between the observed traffic data and the predicted traffic data using Prophet.\n","\n","The trend is fairly accurate, but there are some remarkable issues: for example the traffic peak are not predicted by the alghoritm and, moreover, there are negative values in traffic counts which is physically impossible.\n","\n","In addition, from the statistical analysis, it is clear that the local differences are inaccurate as the time-series increases in time."],"metadata":{"id":"sokk0Li2sID-"}},{"cell_type":"code","source":["fig, axs = plt.subplots(2,1, figsize=(20,8), sharex=True)\n","ax3 = axs[0]\n","ax3.set_title(f'Comparison between Observed Traffic and Predicted Traffic Data')\n","ax3.plot(ts3, df_compared.y,'-', c='blue', label='Observed Traffic Data')\n","ax3.plot(ts3, df_compared.pred,'-.', c='red', label='Predicted Traffic Data [PROPHET]')\n","ax3.set_ylabel('Counter [# of vehicles]')\n","ax3.grid(which='major', linestyle='-')\n","ax3.grid(which='minor', linestyle='--')\n","ax3.legend()\n","\n","ax3 = axs[1]\n","ax3.set_title(f'Statistical Analysis')\n","ax3.set_xticks(ts3[::tick_us3])\n","ax3.set_xticklabels(ts_lbl3[::tick_us3], rotation=45, ha='right')\n","ax3.plot(ts3, df_compared['loc_diff'],'-o', c='blue', label='Differences')\n","ax3.axhspan(ax3.get_ylim()[0], thresh_down2, facecolor=kpi_colors[cg.LOW] , alpha=0.3, label=f'LOW < {kpi_thresh[cg.LOW]} centile')\n","ax3.axhspan(thresh_down2, thresh_up, facecolor=kpi_colors[cg.AVE] , alpha=0.3)\n","ax3.axhspan(thresh_up2, ax3.get_ylim()[1], facecolor=kpi_colors[cg.HIGH] , alpha=0.3, label=f'HIGH > {kpi_thresh[cg.HIGH]} centile')\n","\n","ax3.grid()\n","ax3.legend()\n","ax3.set_xlabel(f'Daytime [Wday Mon DD YYYY HH:MM]')\n","ax3.set_ylabel('Local Differences')\n","plt.suptitle(f'Prophet Results', size=18 )\n","plt.show()"],"metadata":{"id":"6Q3JE29j00E-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusions"],"metadata":{"id":"MaAQ5YXy_fXD"}},{"cell_type":"markdown","source":["In this notebook, starting from a real data set of traffic mobility fluxes  provided by Regione Emilia-Romagna, I have applied two machine learning algorithms in order to get a forcasting of the number of vehicles in the road network.\n","\n","The first model that has been used is based on the Artificial Neural Network (ANN) achitecture. The population data has been divided into two sets: the first (80%) has been used for training, while the second set (20%) has been used as the test dataset.\n","The R-squared score in this case is very accurate and present a value of 98%.\n","\n","Moreover, by fitting the data population of the observed data with the forecast prediction from the multilayer perceptron alghoritm a very good matching has been shown.\n","\n","The downside of this procedure is that it takes time to be completed.\n","\n","In contrast, the second model  is very efficient in term of execution time since it takes just few second to fit the data and predict a reasonable result. Unfortunately, it can predict the seasonality of the data but it loses some details in forecasting quarterly traffic data. "],"metadata":{"id":"Tz4T-R38_jm4"}}]}